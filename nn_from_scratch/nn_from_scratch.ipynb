{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Network from Scratch**\n",
    "\n",
    "---\n",
    "\n",
    "The goal is to implement a complete neural network from scratch with only dense and activation layers. I will be training the network on the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Contents**\n",
    "\n",
    "1. [The Plan](#1.-The-Plan)\n",
    "2. [Creating the base layer](#2.-Creating-the-base-layer)\n",
    "3. [Creating the dense layer](#3.-Creating-the-dense-layer)\n",
    "4. [Creating the activation layer](#4.-Creating-the-activation-layer)\n",
    "5. [Implementing activation functions](#5.-Implementing-activation-functions)\n",
    "6. [Implementing loss functions](#6.-Implementing-loss-functions)\n",
    "7. [Solving MNIST](#7.-Solving-MNIST)  \n",
    "   7.1 [Creating the network](#7.1-Creating-the-network)  \n",
    "   7.2 [Testing the network](#7.2-Testing-the-network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. The Plan**\n",
    "\n",
    "---\n",
    "\n",
    "1. Create a class for a layer object with only forward and backword function that are both empty.\n",
    "2. Create a class for a dense layer that inherits from the layer class.\n",
    "3. Create a layer with an activation function.\n",
    "4. Implement activation functions and loss functions.\n",
    "5. Solve MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I will import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Creating the base layer**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a base class for the layers. This class will have an input and output variable that will be used to store the input and output of the layer.\n",
    "\n",
    "The forward and backward function are empty and will be implemented in the child classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Creating the dense layer**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a dense layer that inherits from the layer class. The dense layer will have a weight and bias variable that will be used to store the weights and bias of the layer.\n",
    "\n",
    "The forward function will take an input and return the output of the layer. The output is calculated by taking the dot product of the input and the weight and adding the bias.\n",
    "\n",
    "```python\n",
    "def forward(self, input):\n",
    "    self.input = input\n",
    "    return np.dot(input, self.weight) + self.bias\n",
    "```\n",
    "\n",
    "The backward function will take a gradient and return the gradient of the input. The gradient of the input is calculated by taking the dot product of the gradient and the weight. The weights and bias are also updated using the gradient.\n",
    "\n",
    "```python\n",
    "def backward(self, output_gradient, learning_rate):\n",
    "    weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "    input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "    self.weights -= learning_rate * weights_gradient\n",
    "    self.bias -= learning_rate * output_gradient\n",
    "    return input_gradient\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Creating the activation layer**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create an activation layer that inherits from the layer class. The activation layer will have an activation and activation_prime variable that will be used to store the activation function and its derivative.\n",
    "\n",
    "> The derivate tells us the rate of change of the activation function at a certain point. This is used to calculate the gradient of the input.\n",
    "\n",
    "The forward function will take an input and return the output of the layer. The output is calculated by applying the activation function to the input.\n",
    "\n",
    "```python\n",
    "def forward(self, input):\n",
    "    self.input = input\n",
    "    return self.activation(input)\n",
    "```\n",
    "\n",
    "The backward function will take a gradient and return the gradient of the input. The gradient of the input is calculated by applying the derivative of the activation function to the gradient.\n",
    "\n",
    "```python\n",
    "def backward(self, output_gradient, learning_rate):\n",
    "    return np.multiply(output_gradient, self.activation_prime(self.input))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Implementing activation functions**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the implementations of the tanh and softmax activation functions. The tanh function is used as the activation function for the hidden layers and the softmax function is used as the activation function for the output layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Implementing loss functions**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have implemented the mean squared error loss function. The loss function takes the true and predicted values and returns the mean squared error. The derivative of the loss function takes the true and predicted values and returns the gradient of the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Solving MNIST**\n",
    "\n",
    "[Table of Contents](#Table-of-Contents)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **7.1 Creating the network**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # Forward.\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # Error.\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # Backward.\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I have implemented the predict and train functions. The predict function takes a model and input and returns the output of the model. The train function takes a network, loss function, loss function derivative, input, output, epochs, learning rate, and verbose and trains the network on the input and output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Testing the network**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I have tested the network on the MNIST dataset. The network was trained on 1000 samples and tested on 20 samples. The output of the network was compared to the true values and printed to the console. The accuracy of the network is shown below.\n",
    "\n",
    "The preprocess_data function is used preprocess the input and output data. It reshapes the input data x into a 2D array with each row representing an image and each column representing a pixel. The pixel intensities are then normalized by dividing by 255 to bring them into the range [0, 1]. The to_categorical function is used to convert the output data y into a one-hot encoded matrix. It returns the first `limit` number of images and labels from the MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.14131456196100928\n",
      "2/100, error=0.1281179909229619\n",
      "3/100, error=0.11820417521838031\n",
      "4/100, error=0.11070249217766505\n",
      "5/100, error=0.10487852202268258\n",
      "6/100, error=0.10063131666280356\n",
      "7/100, error=0.09626106259373281\n",
      "8/100, error=0.09140230224424824\n",
      "9/100, error=0.08741980521998745\n",
      "10/100, error=0.08377758956217897\n",
      "11/100, error=0.08043449369946196\n",
      "12/100, error=0.07753578591514661\n",
      "13/100, error=0.07494677168794342\n",
      "14/100, error=0.07141283999120455\n",
      "15/100, error=0.06816689964832788\n",
      "16/100, error=0.06502347167172204\n",
      "17/100, error=0.06153873985104281\n",
      "18/100, error=0.05701315785895945\n",
      "19/100, error=0.05292273729074436\n",
      "20/100, error=0.04951750740229359\n",
      "21/100, error=0.04681684754605347\n",
      "22/100, error=0.044698000657800406\n",
      "23/100, error=0.04292580928427874\n",
      "24/100, error=0.041183768194564076\n",
      "25/100, error=0.03961781683957099\n",
      "26/100, error=0.03815696846172812\n",
      "27/100, error=0.03680524693473054\n",
      "28/100, error=0.03557785159153293\n",
      "29/100, error=0.034517921432361175\n",
      "30/100, error=0.03359160127135983\n",
      "31/100, error=0.03268882533087786\n",
      "32/100, error=0.03192073098566523\n",
      "33/100, error=0.031200572533228433\n",
      "34/100, error=0.030535146463775982\n",
      "35/100, error=0.029818525101973522\n",
      "36/100, error=0.02910360924764615\n",
      "37/100, error=0.028416494013245443\n",
      "38/100, error=0.027856183461903685\n",
      "39/100, error=0.02710167774240088\n",
      "40/100, error=0.02637279301973739\n",
      "41/100, error=0.025754077561156784\n",
      "42/100, error=0.02526677775814639\n",
      "43/100, error=0.02480735633042434\n",
      "44/100, error=0.02435496060981765\n",
      "45/100, error=0.023784860576756103\n",
      "46/100, error=0.023303136149352945\n",
      "47/100, error=0.02283004677195358\n",
      "48/100, error=0.02223200230039612\n",
      "49/100, error=0.02162755616647172\n",
      "50/100, error=0.02120740780104432\n",
      "51/100, error=0.020775178284503506\n",
      "52/100, error=0.020290853484036416\n",
      "53/100, error=0.019878567424491827\n",
      "54/100, error=0.01942315140760135\n",
      "55/100, error=0.01890350986952827\n",
      "56/100, error=0.018464481682706802\n",
      "57/100, error=0.018120016410114154\n",
      "58/100, error=0.01783337831853766\n",
      "59/100, error=0.01758546854303216\n",
      "60/100, error=0.01735590581826712\n",
      "61/100, error=0.01713222894987084\n",
      "62/100, error=0.016901102652741584\n",
      "63/100, error=0.01665480069876611\n",
      "64/100, error=0.01637861711112596\n",
      "65/100, error=0.016107233794084394\n",
      "66/100, error=0.015839647198339496\n",
      "67/100, error=0.015606836249919886\n",
      "68/100, error=0.015378548553551733\n",
      "69/100, error=0.014997760559923358\n",
      "70/100, error=0.014776540782998723\n",
      "71/100, error=0.014541183973283864\n",
      "72/100, error=0.014344141119045735\n",
      "73/100, error=0.014169351820171688\n",
      "74/100, error=0.013999456883358324\n",
      "75/100, error=0.013859368962357023\n",
      "76/100, error=0.013741013729827879\n",
      "77/100, error=0.013631894461952253\n",
      "78/100, error=0.013526780841240682\n",
      "79/100, error=0.013418064687427438\n",
      "80/100, error=0.013298302719961545\n",
      "81/100, error=0.01314961329516015\n",
      "82/100, error=0.012934736115807996\n",
      "83/100, error=0.012759611888734541\n",
      "84/100, error=0.012622988531095477\n",
      "85/100, error=0.012500551323301475\n",
      "86/100, error=0.012386349315376506\n",
      "87/100, error=0.012273822365300302\n",
      "88/100, error=0.012178326870005555\n",
      "89/100, error=0.012092164283728965\n",
      "90/100, error=0.012013377786031924\n",
      "91/100, error=0.01194127140775615\n",
      "92/100, error=0.011873570882214721\n",
      "93/100, error=0.01180826367471281\n",
      "94/100, error=0.011743207122484391\n",
      "95/100, error=0.011678946679130733\n",
      "96/100, error=0.01161753945179894\n",
      "97/100, error=0.01155962933085916\n",
      "98/100, error=0.011503337858592541\n",
      "99/100, error=0.011445484762607034\n",
      "100/100, error=0.011380275118330428\n",
      "pred: 7 \ttrue: 7\n",
      "pred: 2 \ttrue: 2\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 4 \ttrue: 4\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 9 \ttrue: 4\n",
      "pred: 9 \ttrue: 9\n",
      "pred: 4 \ttrue: 5\n",
      "pred: 7 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 3 \ttrue: 6\n",
      "pred: 4 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 0 \ttrue: 5\n",
      "pred: 7 \ttrue: 9\n",
      "pred: 7 \ttrue: 7\n",
      "pred: 3 \ttrue: 3\n",
      "pred: 4 \ttrue: 4\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    x = x.reshape(x.shape[0], 28 * 28, 1)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    y = to_categorical(y)\n",
    "    y = y.reshape(y.shape[0], 10, 1)\n",
    "    return x[:limit], y[:limit]\n",
    "\n",
    "\n",
    "# load MNIST from server\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 20)\n",
    "\n",
    "network = [\n",
    "    Dense(28 * 28, 40),\n",
    "    Tanh(),\n",
    "    Dense(40, 10),\n",
    "    Softmax()\n",
    "]\n",
    "\n",
    "train(network, mse, mse_prime, x_train, y_train, epochs=100, learning_rate=0.1)\n",
    "\n",
    "# Loop over the test set, and print the prediction and the true value. Zip is used to iterate over two lists at the same time.\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    print('pred:', np.argmax(output), '\\ttrue:', np.argmax(y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the network is 65% which is not very good. This is likely due to the fact that the network is not very deep and does not have many layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 13 / 20 = 65.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    if np.argmax(output) == np.argmax(y):\n",
    "        correct += 1\n",
    "print(f\"Accuracy: {correct} / {len(x_test)} = {correct / len(x_test) * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
